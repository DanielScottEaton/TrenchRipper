{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing from multiple sources\n",
    "\n",
    "Intermediate goal: multiple inputs - > single train, test, val files\n",
    "(no augmentation yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import torch\n",
    "import copy\n",
    "import ipywidgets as ipyw\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "import itertools\n",
    "import qgrid\n",
    "import shutil\n",
    "import subprocess\n",
    "\n",
    "from random import shuffle\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from scipy.ndimage.interpolation import map_coordinates\n",
    "from scipy.interpolate import RectBivariateSpline\n",
    "from scipy import interpolate, ndimage\n",
    "import paulssonlab.deaton.trenchripper.trenchripper as tr\n",
    "\n",
    "import skimage as sk\n",
    "import pickle as pkl\n",
    "import skimage.morphology\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from paulssonlab.deaton.trenchripper.trenchripper import (\n",
    "    pandas_hdf5_handler,\n",
    "    kymo_handle,\n",
    "    writedir,\n",
    ")\n",
    "from paulssonlab.deaton.trenchripper.trenchripper import hdf5lock\n",
    "from paulssonlab.deaton.trenchripper.trenchripper import object_f_scores\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller = tr.dask_controller(\n",
    "    walltime=\"04:00:00\",\n",
    "    local=False,\n",
    "    n_workers=40,\n",
    "    memory=\"2GB\",\n",
    "    working_directory=\"/n/scratch2/de64/nntest7/dask\",\n",
    ")\n",
    "dask_controller.startdask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.daskclient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_controller.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_augmentation:\n",
    "    def __init__(self, mode=\"a\", p_flip=0.5, max_rot=10, min_padding=20):\n",
    "        if mode not in [\"a\", \"m\", \"w\"]:\n",
    "            raise ValueError(\"Not a valid augmentation mode\")\n",
    "        self.mode = mode\n",
    "        self.p_flip = p_flip\n",
    "        self.max_rot = max_rot\n",
    "        self.min_padding = min_padding\n",
    "\n",
    "    def random_crop(self, img_arr, seg_arr):\n",
    "        false_arr = np.zeros(img_arr.shape[2:4], dtype=bool)\n",
    "        random_crop_len_y = np.random.uniform(\n",
    "            low=0.3, high=1.0, size=(1, img_arr.shape[0])\n",
    "        )\n",
    "        random_crop_len_x = np.random.uniform(\n",
    "            low=0.5, high=1.0, size=(1, img_arr.shape[0])\n",
    "        )\n",
    "\n",
    "        random_crop_len = np.concatenate([random_crop_len_y, random_crop_len_x], axis=0)\n",
    "\n",
    "        random_crop_remainder = 1.0 - random_crop_len\n",
    "        random_crop_start = (\n",
    "            np.random.uniform(low=0.0, high=1.0, size=(2, img_arr.shape[0]))\n",
    "        ) * random_crop_remainder\n",
    "        low_crop = np.floor(\n",
    "            random_crop_start * np.array(img_arr.shape[2:4])[:, np.newaxis]\n",
    "        ).astype(\"int32\")\n",
    "        high_crop = np.floor(\n",
    "            low_crop + (random_crop_len * np.array(img_arr.shape[2:4])[:, np.newaxis])\n",
    "        ).astype(\"int32\")\n",
    "\n",
    "        out_arr = []\n",
    "        out_seg_arr = []\n",
    "        center = (img_arr.shape[2] // 2, img_arr.shape[3] // 2)\n",
    "        for t in range(img_arr.shape[0]):\n",
    "            mask = copy.copy(false_arr)\n",
    "            working_arr = copy.copy(img_arr[t, 0, :, :])\n",
    "            working_seg_arr = copy.copy(seg_arr[t, 0, :, :])\n",
    "\n",
    "            dim_0_range = high_crop[0, t] - low_crop[0, t]\n",
    "            dim_1_range = high_crop[1, t] - low_crop[1, t]\n",
    "            top_left = (center[0] - dim_0_range // 2, center[1] - dim_1_range // 2)\n",
    "\n",
    "            dim_0_maxscale = img_arr.shape[2] / dim_0_range\n",
    "            dim_1_maxscale = img_arr.shape[3] / dim_1_range\n",
    "\n",
    "            dim_0_scale = np.clip(\n",
    "                np.random.normal(loc=1.0, scale=0.1), 0.8, dim_0_maxscale\n",
    "            )\n",
    "            dim_1_scale = np.clip(\n",
    "                np.random.normal(loc=1.0, scale=0.1), 0.8, dim_1_maxscale\n",
    "            )\n",
    "\n",
    "            #             dim_0_scale = 1.\n",
    "            #             dim_1_scale = 1.\n",
    "\n",
    "            rescaled_img = sk.transform.rescale(\n",
    "                working_arr[\n",
    "                    low_crop[0, t] : high_crop[0, t], low_crop[1, t] : high_crop[1, t]\n",
    "                ],\n",
    "                (dim_0_scale, dim_1_scale),\n",
    "                preserve_range=True,\n",
    "            ).astype(int)\n",
    "            rescaled_seg = (\n",
    "                sk.transform.rescale(\n",
    "                    working_seg_arr[\n",
    "                        low_crop[0, t] : high_crop[0, t],\n",
    "                        low_crop[1, t] : high_crop[1, t],\n",
    "                    ]\n",
    "                    == 1,\n",
    "                    (dim_0_scale, dim_1_scale),\n",
    "                )\n",
    "                > 0.5\n",
    "            ).astype(\"int8\")\n",
    "\n",
    "            if self.mode is \"m\":\n",
    "                rescaled_border = (\n",
    "                    sk.transform.rescale(\n",
    "                        working_seg_arr[\n",
    "                            low_crop[0, t] : high_crop[0, t],\n",
    "                            low_crop[1, t] : high_crop[1, t],\n",
    "                        ]\n",
    "                        == 2,\n",
    "                        (dim_0_scale, dim_1_scale),\n",
    "                    )\n",
    "                    > 0.5\n",
    "                )\n",
    "                rescaled_seg[rescaled_border] = 2\n",
    "\n",
    "            top_left = (\n",
    "                center[0] - rescaled_img.shape[0] // 2,\n",
    "                center[1] - rescaled_img.shape[1] // 2,\n",
    "            )\n",
    "            working_arr[\n",
    "                top_left[0] : top_left[0] + rescaled_img.shape[0],\n",
    "                top_left[1] : top_left[1] + rescaled_img.shape[1],\n",
    "            ] = rescaled_img\n",
    "            working_seg_arr[\n",
    "                top_left[0] : top_left[0] + rescaled_img.shape[0],\n",
    "                top_left[1] : top_left[1] + rescaled_img.shape[1],\n",
    "            ] = rescaled_seg\n",
    "\n",
    "            mask[\n",
    "                top_left[0] : top_left[0] + rescaled_img.shape[0],\n",
    "                top_left[1] : top_left[1] + rescaled_img.shape[1],\n",
    "            ] = True\n",
    "            working_arr[~mask] = 0\n",
    "            working_seg_arr[~mask] = False\n",
    "\n",
    "            out_arr.append(working_arr)\n",
    "            out_seg_arr.append(working_seg_arr)\n",
    "        out_arr = np.expand_dims(np.array(out_arr), 1)\n",
    "        out_seg_arr = np.expand_dims(np.array(out_seg_arr), 1)\n",
    "        return out_arr, out_seg_arr\n",
    "\n",
    "    def random_x_flip(self, img_arr, seg_arr, p=0.5):\n",
    "        choices = np.random.choice(\n",
    "            np.array([True, False]), size=img_arr.shape[0], p=np.array([p, 1.0 - p])\n",
    "        )\n",
    "        out_img_arr = copy.copy(img_arr)\n",
    "        out_seg_arr = copy.copy(seg_arr)\n",
    "        out_img_arr[choices, 0, :, :] = np.flip(img_arr[choices, 0, :, :], axis=1)\n",
    "        out_seg_arr[choices, 0, :, :] = np.flip(seg_arr[choices, 0, :, :], axis=1)\n",
    "        return out_img_arr, out_seg_arr\n",
    "\n",
    "    def random_y_flip(self, img_arr, seg_arr, p=0.5):\n",
    "        choices = np.random.choice(\n",
    "            np.array([True, False]), size=img_arr.shape[0], p=np.array([p, 1.0 - p])\n",
    "        )\n",
    "        out_img_arr = copy.copy(img_arr)\n",
    "        out_seg_arr = copy.copy(seg_arr)\n",
    "        out_img_arr[choices, 0, :, :] = np.flip(img_arr[choices, 0, :, :], axis=2)\n",
    "        out_seg_arr[choices, 0, :, :] = np.flip(seg_arr[choices, 0, :, :], axis=2)\n",
    "        return out_img_arr, out_seg_arr\n",
    "\n",
    "    def change_brightness(self, img_arr, num_control_points=3):\n",
    "        out_img_arr = copy.copy(img_arr)\n",
    "        for t in range(img_arr.shape[0]):\n",
    "            control_points = (\n",
    "                np.add.accumulate(np.ones(num_control_points + 2)) - 1.0\n",
    "            ) / (num_control_points + 1)\n",
    "            control_point_locations = (control_points * 65535).astype(int)\n",
    "            orig_locations = copy.copy(control_point_locations)\n",
    "            random_points = np.random.uniform(\n",
    "                low=0, high=65535, size=num_control_points\n",
    "            ).astype(int)\n",
    "            sorted_points = np.sort(random_points)\n",
    "            control_point_locations[1:-1] = sorted_points\n",
    "            mapping = interpolate.PchipInterpolator(\n",
    "                orig_locations, control_point_locations\n",
    "            )\n",
    "            out_img_arr[t, 0, :, :] = mapping(img_arr[t, 0, :, :])\n",
    "        return out_img_arr\n",
    "\n",
    "    def add_padding(self, img_arr, seg_arr, max_rot=20, min_padding=20):\n",
    "        hyp_length = np.ceil(\n",
    "            (img_arr.shape[2] ** 2 + img_arr.shape[3] ** 2) ** (1 / 2)\n",
    "        ).astype(int)\n",
    "        max_rads = ((90 - max_rot) / 360) * (2 * np.pi)\n",
    "        min_rads = (90 / 360) * (2 * np.pi)\n",
    "        max_y = np.maximum(\n",
    "            np.ceil(hyp_length * np.sin(max_rads)),\n",
    "            np.ceil(hyp_length * np.sin(min_rads)),\n",
    "        ).astype(int)\n",
    "        max_x = np.maximum(\n",
    "            np.ceil(hyp_length * np.cos(max_rads)),\n",
    "            np.ceil(hyp_length * np.cos(min_rads)),\n",
    "        ).astype(int)\n",
    "        delta_y = max_y - img_arr.shape[2]\n",
    "        delta_x = max_x - img_arr.shape[3]\n",
    "        if delta_x % 2 == 1:\n",
    "            delta_x += 1\n",
    "        if delta_y % 2 == 1:\n",
    "            delta_y += 1\n",
    "        delta_y = np.maximum(delta_y, 2 * min_padding)\n",
    "        delta_x = np.maximum(delta_x, 2 * min_padding)\n",
    "        padded_img_arr = np.pad(\n",
    "            img_arr,\n",
    "            (\n",
    "                (0, 0),\n",
    "                (0, 0),\n",
    "                (delta_y // 2, delta_y // 2),\n",
    "                (delta_x // 2, delta_x // 2),\n",
    "            ),\n",
    "            \"constant\",\n",
    "            constant_values=0,\n",
    "        )\n",
    "        padded_seg_arr = np.pad(\n",
    "            seg_arr,\n",
    "            (\n",
    "                (0, 0),\n",
    "                (0, 0),\n",
    "                (delta_y // 2, delta_y // 2),\n",
    "                (delta_x // 2, delta_x // 2),\n",
    "            ),\n",
    "            \"constant\",\n",
    "            constant_values=0,\n",
    "        )\n",
    "        return padded_img_arr, padded_seg_arr\n",
    "\n",
    "    def translate(self, pad_img_arr, pad_seg_arr, img_arr, seg_arr):\n",
    "        trans_img_arr = copy.copy(pad_img_arr)\n",
    "        trans_seg_arr = copy.copy(pad_seg_arr)\n",
    "        delta_y = pad_img_arr.shape[2] - img_arr.shape[2]\n",
    "        delta_x = pad_img_arr.shape[3] - img_arr.shape[3]\n",
    "        for t in range(pad_img_arr.shape[0]):\n",
    "            trans_y = np.random.randint(-(delta_y // 2), high=delta_y // 2)\n",
    "            trans_x = np.random.randint(-(delta_x // 2), high=delta_x // 2)\n",
    "            trans_img_arr[\n",
    "                t,\n",
    "                0,\n",
    "                delta_y // 2 : delta_y // 2 + img_arr.shape[2],\n",
    "                delta_x // 2 : delta_x // 2 + img_arr.shape[3],\n",
    "            ] = 0\n",
    "            trans_seg_arr[\n",
    "                t,\n",
    "                0,\n",
    "                delta_y // 2 : delta_y // 2 + img_arr.shape[2],\n",
    "                delta_x // 2 : delta_x // 2 + img_arr.shape[3],\n",
    "            ] = 0\n",
    "            trans_img_arr[\n",
    "                t,\n",
    "                0,\n",
    "                delta_y // 2 + trans_y : delta_y // 2 + img_arr.shape[2] + trans_y,\n",
    "                delta_x // 2 + trans_x : delta_x // 2 + img_arr.shape[3] + trans_x,\n",
    "            ] = pad_img_arr[\n",
    "                t,\n",
    "                0,\n",
    "                delta_y // 2 : delta_y // 2 + img_arr.shape[2],\n",
    "                delta_x // 2 : delta_x // 2 + img_arr.shape[3],\n",
    "            ]\n",
    "            trans_seg_arr[\n",
    "                t,\n",
    "                0,\n",
    "                delta_y // 2 + trans_y : delta_y // 2 + img_arr.shape[2] + trans_y,\n",
    "                delta_x // 2 + trans_x : delta_x // 2 + img_arr.shape[3] + trans_x,\n",
    "            ] = pad_seg_arr[\n",
    "                t,\n",
    "                0,\n",
    "                delta_y // 2 : delta_y // 2 + img_arr.shape[2],\n",
    "                delta_x // 2 : delta_x // 2 + img_arr.shape[3],\n",
    "            ]\n",
    "        return trans_img_arr, trans_seg_arr\n",
    "\n",
    "    def rotate(self, img_arr, seg_arr, max_rot=20):\n",
    "        rot_img_arr = copy.copy(img_arr)\n",
    "        rot_seg_arr = copy.copy(seg_arr)\n",
    "        for t in range(img_arr.shape[0]):\n",
    "            r = np.random.uniform(low=-max_rot, high=max_rot)\n",
    "            rot_img_arr[t, 0, :, :] = sk.transform.rotate(\n",
    "                img_arr[t, 0, :, :], r, preserve_range=True\n",
    "            ).astype(\"int32\")\n",
    "            rot_seg = (sk.transform.rotate(seg_arr[t, 0, :, :] == 1, r) > 0.5).astype(\n",
    "                \"int8\"\n",
    "            )\n",
    "            if self.mode is \"m\":\n",
    "                rot_border = sk.transform.rotate(seg_arr[t, 0, :, :] == 2, r) > 0.5\n",
    "                rot_seg[rot_border] = 2\n",
    "            rot_seg_arr[t, 0, :, :] = rot_seg\n",
    "        return rot_img_arr, rot_seg_arr\n",
    "\n",
    "    def deform_img_arr(self, img_arr, seg_arr):\n",
    "        def_img_arr = copy.copy(img_arr)\n",
    "        def_seg_arr = copy.copy(seg_arr)\n",
    "        for t in range(img_arr.shape[0]):\n",
    "            y_steps = np.linspace(0.0, 4.0, num=img_arr.shape[2])\n",
    "            x_steps = np.linspace(0.0, 4.0, num=img_arr.shape[3])\n",
    "            grid = np.random.normal(scale=1.0, size=(2, 4, 4))\n",
    "            dx = RectBivariateSpline(np.arange(4), np.arange(4), grid[0]).ev(\n",
    "                y_steps[:, np.newaxis], x_steps[np.newaxis, :]\n",
    "            )\n",
    "            dy = RectBivariateSpline(np.arange(4), np.arange(4), grid[1]).ev(\n",
    "                y_steps[:, np.newaxis], x_steps[np.newaxis, :]\n",
    "            )\n",
    "            y, x = np.meshgrid(\n",
    "                np.arange(img_arr.shape[2]), np.arange(img_arr.shape[3]), indexing=\"ij\"\n",
    "            )\n",
    "            indices = np.reshape(y + dy, (-1, 1)), np.reshape(x + dx, (-1, 1))\n",
    "            elastic_img = map_coordinates(\n",
    "                img_arr[t, 0, :, :], indices, order=1\n",
    "            ).reshape(img_arr.shape[2:4])\n",
    "\n",
    "            def_img_arr[t, 0, :, :] = elastic_img\n",
    "\n",
    "            elastic_cell = map_coordinates(\n",
    "                seg_arr[t, 0, :, :] == 1, indices, order=1\n",
    "            ).reshape(seg_arr.shape[2:4])\n",
    "            elastic_cell = sk.morphology.binary_closing(elastic_cell)\n",
    "            def_seg_arr[t, 0, elastic_cell] = 1\n",
    "            if self.mode is \"m\":\n",
    "                elastic_border = map_coordinates(\n",
    "                    seg_arr[t, 0, :, :] == 2, indices, order=1\n",
    "                ).reshape(seg_arr.shape[2:4])\n",
    "                def_seg_arr[t, 0, elastic_border] = 2\n",
    "        return def_img_arr, def_seg_arr\n",
    "\n",
    "    def add_borders(self, seg_arr):\n",
    "        output_arr = copy.copy(seg_arr)\n",
    "        for k in range(seg_arr.shape[0]):\n",
    "            working_mask = seg_arr[k, 0] == 1\n",
    "            expanded = sk.morphology.binary_dilation(working_mask)\n",
    "            border = expanded ^ working_mask\n",
    "            output_arr[k, 0, border] = 2\n",
    "        return output_arr\n",
    "\n",
    "    def repair_borders(self, seg_arr):\n",
    "        output_arr = copy.copy(seg_arr)\n",
    "        output_arr[output_arr == 2] = 0\n",
    "        output_arr = self.add_borders(output_arr)\n",
    "        return output_arr\n",
    "\n",
    "    def get_augmented_data(self, img_arr, seg_arr):\n",
    "        img_arr, seg_arr = (img_arr.astype(\"int32\"), seg_arr.astype(\"int8\"))\n",
    "        if self.mode is \"m\":\n",
    "            seg_arr = self.add_borders(seg_arr)\n",
    "        img_arr, seg_arr = self.random_crop(img_arr, seg_arr)\n",
    "        img_arr, seg_arr = self.random_x_flip(img_arr, seg_arr, p=self.p_flip)\n",
    "        img_arr, seg_arr = self.random_y_flip(img_arr, seg_arr, p=self.p_flip)\n",
    "        img_arr = self.change_brightness(img_arr)\n",
    "        pad_img_arr, pad_seg_arr = self.add_padding(\n",
    "            img_arr, seg_arr, max_rot=self.max_rot + 5\n",
    "        )\n",
    "        img_arr, seg_arr = self.translate(pad_img_arr, pad_seg_arr, img_arr, seg_arr)\n",
    "        del pad_img_arr\n",
    "        del pad_seg_arr\n",
    "        img_arr, seg_arr = self.rotate(img_arr, seg_arr, max_rot=self.max_rot)\n",
    "        img_arr, seg_arr = self.deform_img_arr(img_arr, seg_arr)\n",
    "        seg_arr = self.repair_borders(seg_arr)\n",
    "        #         img_arr,seg_arr = (img_arr.astype(\"int32\"),seg_arr.astype(\"int8\"))\n",
    "        return img_arr, seg_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = data_augmentation(mode=\"m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 22\n",
    "t = 10\n",
    "\n",
    "with h5py.File(\n",
    "    \"/n/scratch2/de64/2019-05-31_validation_data/kymograph/kymograph_0.hdf5\", \"r\"\n",
    ") as infile:\n",
    "    img = infile[\"Phase\"][k, t]\n",
    "\n",
    "with h5py.File(\n",
    "    \"/n/scratch2/de64/2019-05-31_validation_data/fluorsegmentation/segmentation_0.hdf5\",\n",
    "    \"r\",\n",
    ") as infile:\n",
    "    seg = infile[\"data\"][k, t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.subplot(1, 4, 1)\n",
    "ax.imshow(img)\n",
    "ax = plt.subplot(1, 4, 2)\n",
    "ax.imshow(seg)\n",
    "ax = plt.subplot(1, 4, 3)\n",
    "img_arr, seg_arr = test.get_augmented_data(\n",
    "    img[np.newaxis, np.newaxis], seg[np.newaxis, np.newaxis]\n",
    ")\n",
    "ax.imshow(img_arr[0, 0])\n",
    "ax = plt.subplot(1, 4, 4)\n",
    "ax.imshow(seg_arr[0, 0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class weightmap_generator:\n",
    "    def __init__(self, mode, w0=0.0, wm_sigma=0.0):\n",
    "        if mode not in [\"a\", \"m\", \"w\"]:\n",
    "            raise ValueError(\"Not a valid augmentation mode\")\n",
    "        self.mode = mode\n",
    "        self.w0 = w0\n",
    "        self.wm_sigma = wm_sigma\n",
    "\n",
    "    def make_one_class_weightmap(self, single_mask):\n",
    "        ttl_count = single_mask.size\n",
    "\n",
    "        backround_mask = single_mask == 0\n",
    "        cell_mask = single_mask == 1\n",
    "\n",
    "        background_count = np.sum(backround_mask)\n",
    "        cell_count = np.sum(cell_mask)\n",
    "\n",
    "        class_weight = np.array(\n",
    "            [ttl_count / (background_count + 1), ttl_count / (cell_count + 1)]\n",
    "        )\n",
    "        class_weight = class_weight / np.sum(class_weight)\n",
    "\n",
    "        weight_map = np.zeros(single_mask.shape, dtype=float)\n",
    "        weight_map[backround_mask] = class_weight[0]\n",
    "        weight_map[cell_mask] = class_weight[1]\n",
    "\n",
    "        return weight_map\n",
    "\n",
    "    def make_two_class_weightmap(self, single_mask):\n",
    "        ttl_count = single_mask.size\n",
    "\n",
    "        backround_mask = single_mask == 0\n",
    "        cell_mask = single_mask == 1\n",
    "        border_mask = single_mask == 2\n",
    "\n",
    "        background_count = np.sum(backround_mask)\n",
    "        cell_count = np.sum(cell_mask)\n",
    "        border_count = np.sum(border_mask)\n",
    "\n",
    "        class_weight = np.array(\n",
    "            [\n",
    "                ttl_count / (background_count + 1),\n",
    "                ttl_count / (cell_count + 1),\n",
    "                ttl_count / (border_count + 1),\n",
    "            ]\n",
    "        )\n",
    "        class_weight = class_weight / np.sum(class_weight)\n",
    "\n",
    "        weight_map = np.zeros(single_mask.shape, dtype=float)\n",
    "        weight_map[backround_mask] = class_weight[0]\n",
    "        weight_map[cell_mask] = class_weight[1]\n",
    "        weight_map[border_mask] = class_weight[2]\n",
    "\n",
    "        return weight_map\n",
    "\n",
    "    def make_unet_weight_map(self, single_mask):\n",
    "        binary_mask = single_mask == 1\n",
    "\n",
    "        ttl_count = binary_mask.size\n",
    "        cell_count = np.sum(binary_mask == 1)\n",
    "        background_count = ttl_count - cell_count\n",
    "        class_weight = np.array(\n",
    "            [ttl_count / (background_count + 1), ttl_count / (cell_count + 1)]\n",
    "        )\n",
    "        class_weight = class_weight / np.sum(class_weight)\n",
    "\n",
    "        labeled = sk.measure.label(binary_mask)\n",
    "        labels = np.unique(labeled)[1:]\n",
    "\n",
    "        dist_maps = []\n",
    "        borders = []\n",
    "\n",
    "        num_labels = len(labels)\n",
    "\n",
    "        if num_labels == 0:\n",
    "            weight_map = np.ones(binary_mask.shape) * class_weight[0]\n",
    "        elif num_labels == 1:\n",
    "            cell = labeled == 1\n",
    "            #             dilated = sk.morphology.binary_dilation(cell)\n",
    "            eroded = sk.morphology.binary_dilation(cell)\n",
    "            border = eroded ^ cell\n",
    "            weight_map = np.ones(binary_mask.shape) * class_weight[0]\n",
    "            weight_map[binary_mask] += class_weight[1]\n",
    "        #             weight[border] = 0.\n",
    "        else:\n",
    "            for i in labels:\n",
    "                cell = labeled == i\n",
    "                #                 dilated = sk.morphology.binary_dilation(cell)\n",
    "                eroded = sk.morphology.binary_dilation(cell)\n",
    "                border = eroded ^ cell\n",
    "                borders.append(border)\n",
    "                dist_map = scipy.ndimage.morphology.distance_transform_edt(~border)\n",
    "                dist_maps.append(dist_map)\n",
    "            dist_maps = np.array(dist_maps)\n",
    "            borders = np.array(borders)\n",
    "            borders = np.max(borders, axis=0)\n",
    "            dist_maps = np.sort(dist_maps, axis=0)\n",
    "            weight_map = self.w0 * np.exp(\n",
    "                -((dist_maps[0] + dist_maps[1]) ** 2) / (2 * (self.wm_sigma ** 2))\n",
    "            )\n",
    "            weight_map[binary_mask] += class_weight[1]\n",
    "            weight_map[~binary_mask] += class_weight[0]\n",
    "        #             weight[borders] = 0.\n",
    "        return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet_Training_DataLoader:\n",
    "    def __init__(\n",
    "        self,\n",
    "        nndatapath=\"\",\n",
    "        experimentname=\"\",\n",
    "        output_names=[\"train\", \"test\", \"val\"],\n",
    "        output_modes=[\"a\", \"m\"],\n",
    "        num_epochs=10,\n",
    "        input_paths=[],\n",
    "    ):\n",
    "        self.nndatapath = nndatapath\n",
    "        self.experimentname = experimentname\n",
    "        self.output_names = output_names\n",
    "        self.output_modes = output_modes\n",
    "        self.num_epochs = num_epochs\n",
    "        self.input_paths = input_paths\n",
    "\n",
    "        self.metapath = self.nndatapath + \"/metadata.hdf5\"\n",
    "\n",
    "    def get_metadata(self, headpath):\n",
    "        meta_handle = pandas_hdf5_handler(headpath + \"/metadata.hdf5\")\n",
    "        global_handle = meta_handle.read_df(\"global\", read_metadata=True)\n",
    "        kymo_handle = meta_handle.read_df(\"kymograph\", read_metadata=True)\n",
    "        fovdf = kymo_handle.reset_index(inplace=False)\n",
    "        fovdf = fovdf.set_index(\n",
    "            [\"fov\", \"row\", \"trench\"], drop=True, append=False, inplace=False\n",
    "        )\n",
    "        fovdf = fovdf.sort_index()\n",
    "\n",
    "        channel_list = global_handle.metadata[\"channels\"]\n",
    "        fov_list = kymo_handle[\"fov\"].unique().tolist()\n",
    "        t_len = len(kymo_handle.index.get_level_values(\"timepoints\").unique())\n",
    "        ttl_trenches = len(fovdf[\"trenchid\"].unique())\n",
    "        trench_dict = {\n",
    "            fov: len(fovdf.loc[fov][\"trenchid\"].unique()) for fov in fov_list\n",
    "        }\n",
    "        shape_y = kymo_handle.metadata[\"kymograph_params\"][\"ttl_len_y\"]\n",
    "        shape_x = kymo_handle.metadata[\"kymograph_params\"][\"trench_width_x\"]\n",
    "        kymograph_img_shape = tuple((shape_y, shape_x))\n",
    "        return (\n",
    "            channel_list,\n",
    "            fov_list,\n",
    "            t_len,\n",
    "            trench_dict,\n",
    "            ttl_trenches,\n",
    "            kymograph_img_shape,\n",
    "        )\n",
    "\n",
    "    def inter_get_selection(self):\n",
    "        output_tabs = []\n",
    "        for i in range(len(self.output_names)):\n",
    "            dset_tabs = []\n",
    "            for j in range(len(self.input_paths)):\n",
    "                (\n",
    "                    channel_list,\n",
    "                    fov_list,\n",
    "                    t_len,\n",
    "                    trench_dict,\n",
    "                    ttl_trenches,\n",
    "                    kymograph_img_shape,\n",
    "                ) = self.get_metadata(self.input_paths[j])\n",
    "\n",
    "                feature_dropdown = ipyw.Dropdown(\n",
    "                    options=channel_list,\n",
    "                    value=channel_list[0],\n",
    "                    description=\"Feature Channel:\",\n",
    "                    disabled=False,\n",
    "                )\n",
    "                max_samples = ipyw.IntText(\n",
    "                    value=0, description=\"Maximum Samples per Dataset:\", disabled=False\n",
    "                )\n",
    "                t_range = ipyw.IntRangeSlider(\n",
    "                    value=[0, t_len - 1],\n",
    "                    description=\"Timepoint Range:\",\n",
    "                    min=0,\n",
    "                    max=t_len - 1,\n",
    "                    step=1,\n",
    "                    disabled=False,\n",
    "                    continuous_update=False,\n",
    "                )\n",
    "\n",
    "                working_tab = ipyw.VBox(\n",
    "                    children=[feature_dropdown, max_samples, t_range]\n",
    "                )\n",
    "                dset_tabs.append(working_tab)\n",
    "\n",
    "            dset_ipy_tabs = ipyw.Tab(children=dset_tabs)\n",
    "            for j in range(len(self.input_paths)):\n",
    "                dset_ipy_tabs.set_title(j, self.input_paths[j].split(\"/\")[-1])\n",
    "            output_tabs.append(dset_ipy_tabs)\n",
    "        output_ipy_tabs = ipyw.Tab(children=output_tabs)\n",
    "        for i, output_name in enumerate(self.output_names):\n",
    "            output_ipy_tabs.set_title(i, output_name)\n",
    "        self.tab = output_ipy_tabs\n",
    "\n",
    "        return self.tab\n",
    "\n",
    "    def get_import_params(self):\n",
    "        self.import_param_dict = {}\n",
    "        for i, output_name in enumerate(self.output_names):\n",
    "            self.import_param_dict[output_name] = {}\n",
    "            for j, input_path in enumerate(self.input_paths):\n",
    "                working_vbox = self.tab.children[i].children[j]\n",
    "                self.import_param_dict[output_name][input_path] = {\n",
    "                    child.description: child.value for child in working_vbox.children\n",
    "                }\n",
    "\n",
    "        print(\"======== Import Params ========\")\n",
    "        for i, output_name in enumerate(self.output_names):\n",
    "            print(str(output_name))\n",
    "            for j, input_path in enumerate(self.input_paths):\n",
    "                (\n",
    "                    channel_list,\n",
    "                    fov_list,\n",
    "                    t_len,\n",
    "                    trench_dict,\n",
    "                    ttl_trenches,\n",
    "                    kymograph_img_shape,\n",
    "                ) = self.get_metadata(input_path)\n",
    "                ttl_possible_samples = t_len * ttl_trenches\n",
    "                param_dict = self.import_param_dict[output_name][input_path]\n",
    "                requested_samples = param_dict[\"Maximum Samples per Dataset:\"]\n",
    "                if requested_samples > 0:\n",
    "                    print(str(input_path))\n",
    "                    for key, val in param_dict.items():\n",
    "                        print(key + \" \" + str(val))\n",
    "                    print(\n",
    "                        \"Requested Samples / Total Samples: \"\n",
    "                        + str(requested_samples)\n",
    "                        + \"/\"\n",
    "                        + str(ttl_possible_samples)\n",
    "                    )\n",
    "\n",
    "        del self.tab\n",
    "\n",
    "    def export_chunk(self, output_name, init_idx, chunk_size, chunk_idx):\n",
    "        output_meta_handle = pandas_hdf5_handler(self.metapath)\n",
    "        output_df = output_meta_handle.read_df(output_name)\n",
    "        working_df = output_df[init_idx : init_idx + chunk_size]\n",
    "        nndatapath = (\n",
    "            self.nndatapath + \"/\" + output_name + \"_\" + str(chunk_idx) + \".hdf5\"\n",
    "        )\n",
    "\n",
    "        dset_paths = working_df.index.get_level_values(0).unique().tolist()\n",
    "        for dset_path in dset_paths:\n",
    "            dset_path_key = dset_path.split(\"/\")[-1]\n",
    "            dset_df = working_df.loc[dset_path]\n",
    "            param_dict = self.import_param_dict[output_name][dset_path]\n",
    "            feature_channel = param_dict[\"Feature Channel:\"]\n",
    "\n",
    "            img_arr_list = []\n",
    "            seg_arr_list = []\n",
    "\n",
    "            file_indices = dset_df.index.get_level_values(0).unique().tolist()\n",
    "\n",
    "            for file_idx in file_indices:\n",
    "                file_df = dset_df.loc[file_idx]\n",
    "\n",
    "                img_path = dset_path + \"/kymograph/kymograph_\" + str(file_idx) + \".hdf5\"\n",
    "                seg_path = (\n",
    "                    dset_path\n",
    "                    + \"/fluorsegmentation/segmentation_\"\n",
    "                    + str(file_idx)\n",
    "                    + \".hdf5\"\n",
    "                )\n",
    "\n",
    "                with h5py.File(img_path, \"r\") as imgfile:\n",
    "                    working_arr = imgfile[feature_channel][:]\n",
    "\n",
    "                for trench_idx, row in file_df.iterrows():\n",
    "                    img_arr = working_arr[trench_idx, row[\"timepoints\"]][\n",
    "                        np.newaxis, np.newaxis, :, :\n",
    "                    ]  # 1,1,y,x img\n",
    "                    img_arr = img_arr.astype(\"int32\")\n",
    "                    img_arr_list.append(img_arr)\n",
    "\n",
    "                with h5py.File(seg_path, \"r\") as segfile:\n",
    "                    working_arr = segfile[\"data\"][:]\n",
    "\n",
    "                for trench_idx, row in file_df.iterrows():\n",
    "                    seg_arr = working_arr[trench_idx, row[\"timepoints\"]][\n",
    "                        np.newaxis, np.newaxis, :, :\n",
    "                    ]\n",
    "                    seg_arr = seg_arr.astype(\"int8\")\n",
    "                    seg_arr_list.append(seg_arr)\n",
    "\n",
    "            output_img_arr = np.concatenate(img_arr_list, axis=0)\n",
    "            output_seg_arr = np.concatenate(seg_arr_list, axis=0)\n",
    "            chunk_shape = (1, 1, output_img_arr.shape[2], output_img_arr.shape[3])\n",
    "\n",
    "            with h5py.File(nndatapath, \"w\") as outfile:\n",
    "                for output_mode in self.output_modes:\n",
    "                    augmenter = data_augmentation(mode=output_mode)\n",
    "                    for epoch in range(self.num_epochs):\n",
    "                        img_arr, seg_arr = augmenter.get_augmented_data(\n",
    "                            output_img_arr, output_seg_arr\n",
    "                        )\n",
    "                        img_handle = outfile.create_dataset(\n",
    "                            dset_path_key\n",
    "                            + \"/\"\n",
    "                            + output_mode\n",
    "                            + \"/epoch_\"\n",
    "                            + str(epoch)\n",
    "                            + \"/img\",\n",
    "                            data=img_arr,\n",
    "                            chunks=chunk_shape,\n",
    "                            dtype=\"int32\",\n",
    "                        )\n",
    "                        seg_handle = outfile.create_dataset(\n",
    "                            dset_path_key\n",
    "                            + \"/\"\n",
    "                            + output_mode\n",
    "                            + \"/epoch_\"\n",
    "                            + str(epoch)\n",
    "                            + \"/seg\",\n",
    "                            data=seg_arr,\n",
    "                            chunks=chunk_shape,\n",
    "                            dtype=\"int8\",\n",
    "                        )\n",
    "\n",
    "        return init_idx\n",
    "\n",
    "    #         if augment:\n",
    "    #             img_arr,seg_arr = self.data_augmentation.get_augmented_data(img_arr,seg_arr)\n",
    "\n",
    "    #         chunk_shape = (1,1,img_arr.shape[2],img_arr.shape[3])\n",
    "\n",
    "    #         with h5py.File(nndatapath,\"w\") as outfile:\n",
    "    #             img_handle = outfile.create_dataset(\"img\",data=img_arr,chunks=chunk_shape,dtype='int32')\n",
    "    #             seg_handle = outfile.create_dataset(\"seg\",data=seg_arr,chunks=chunk_shape,dtype='int8')\n",
    "\n",
    "    #         for item in weight_grid_list:\n",
    "    #             w0,wm_sigma = item\n",
    "    #             weightmap_gen = weightmap_generator(self.nndatapath,w0,wm_sigma)\n",
    "    #             weightmap_arr = weightmap_gen.make_weightmaps(seg_arr)\n",
    "    #             with h5py.File(nndatapath,\"a\") as outfile:\n",
    "    #                 weightmap_handle = outfile.create_dataset(\"weight_\" + str(item),data=weightmap_arr,chunks=chunk_shape,dtype='int32')\n",
    "\n",
    "    #         return file_idx\n",
    "\n",
    "    def gather_chunks(self, output_name, init_idx_list, chunk_idx_list, chunk_size):\n",
    "\n",
    "        #                       outputdf,output_metadata,selectionname,file_idx_list,weight_grid_list):\n",
    "        nnoutputpath = self.nndatapath + \"/\" + output_name + \".hdf5\"\n",
    "        output_meta_handle = pandas_hdf5_handler(self.metapath)\n",
    "        output_df = output_meta_handle.read_df(output_name)\n",
    "\n",
    "        dset_paths = output_df.index.get_level_values(0).unique().tolist()\n",
    "        for dset_path in dset_paths:\n",
    "            dset_path_key = dset_path.split(\"/\")[-1]\n",
    "            dset_df = output_df.loc[dset_path]\n",
    "\n",
    "            tempdatapath = self.nndatapath + \"/\" + output_name + \"_0.hdf5\"\n",
    "            with h5py.File(tempdatapath, \"r\") as infile:\n",
    "                img_shape = infile[\n",
    "                    dset_path_key + \"/\" + self.output_modes[0] + \"/epoch_0/img\"\n",
    "                ].shape\n",
    "\n",
    "            output_shape = (len(dset_df.index), 1, img_shape[2], img_shape[3])\n",
    "            chunk_shape = (1, 1, img_shape[2], img_shape[3])\n",
    "\n",
    "            with h5py.File(nnoutputpath, \"w\") as outfile:\n",
    "                for output_mode in self.output_modes:\n",
    "                    for epoch in range(self.num_epochs):\n",
    "                        img_handle = outfile.create_dataset(\n",
    "                            dset_path_key\n",
    "                            + \"/\"\n",
    "                            + output_mode\n",
    "                            + \"/epoch_\"\n",
    "                            + str(epoch)\n",
    "                            + \"/img\",\n",
    "                            output_shape,\n",
    "                            chunks=chunk_shape,\n",
    "                            dtype=\"int32\",\n",
    "                        )\n",
    "                        seg_handle = outfile.create_dataset(\n",
    "                            dset_path_key\n",
    "                            + \"/\"\n",
    "                            + output_mode\n",
    "                            + \"/epoch_\"\n",
    "                            + str(epoch)\n",
    "                            + \"/seg\",\n",
    "                            output_shape,\n",
    "                            chunks=chunk_shape,\n",
    "                            dtype=\"int8\",\n",
    "                        )\n",
    "\n",
    "        #             for item in weight_grid_list:\n",
    "        #                 weightmap_handle = outfile.create_dataset(\"weight_\" + str(item),output_shape,chunks=chunk_shape,dtype='int32')\n",
    "        current_dset_path = \"\"\n",
    "        for i, init_idx in enumerate(init_idx_list):\n",
    "            chunk_idx = chunk_idx_list[i]\n",
    "            nndatapath = (\n",
    "                self.nndatapath + \"/\" + output_name + \"_\" + str(chunk_idx) + \".hdf5\"\n",
    "            )\n",
    "            working_df = output_df[init_idx : init_idx + chunk_size]\n",
    "            dset_paths = working_df.index.get_level_values(0).unique().tolist()\n",
    "\n",
    "            with h5py.File(nndatapath, \"r\") as infile:\n",
    "\n",
    "                for dset_path in dset_paths:\n",
    "                    if dset_path != current_dset_path:\n",
    "                        current_idx = 0\n",
    "                        current_dset_path = dset_path\n",
    "\n",
    "                    dset_path_key = dset_path.split(\"/\")[-1]\n",
    "                    dset_df = output_df.loc[dset_path]\n",
    "\n",
    "                    with h5py.File(nnoutputpath, \"a\") as outfile:\n",
    "\n",
    "                        for output_mode in self.output_modes:\n",
    "                            for epoch in range(self.num_epochs):\n",
    "                                img_arr = infile[\n",
    "                                    dset_path_key\n",
    "                                    + \"/\"\n",
    "                                    + output_mode\n",
    "                                    + \"/epoch_\"\n",
    "                                    + str(epoch)\n",
    "                                    + \"/img\"\n",
    "                                ][\n",
    "                                    :\n",
    "                                ]  # k,1,y,x\n",
    "                                seg_arr = infile[\n",
    "                                    dset_path_key\n",
    "                                    + \"/\"\n",
    "                                    + output_mode\n",
    "                                    + \"/epoch_\"\n",
    "                                    + str(epoch)\n",
    "                                    + \"/seg\"\n",
    "                                ][\n",
    "                                    :\n",
    "                                ]  # k,1,y,x\n",
    "                                num_indices = img_arr.shape[0]\n",
    "\n",
    "                                outfile[\n",
    "                                    dset_path_key\n",
    "                                    + \"/\"\n",
    "                                    + output_mode\n",
    "                                    + \"/epoch_\"\n",
    "                                    + str(epoch)\n",
    "                                    + \"/img\"\n",
    "                                ][current_idx : current_idx + num_indices] = img_arr\n",
    "                                outfile[\n",
    "                                    dset_path_key\n",
    "                                    + \"/\"\n",
    "                                    + output_mode\n",
    "                                    + \"/epoch_\"\n",
    "                                    + str(epoch)\n",
    "                                    + \"/seg\"\n",
    "                                ][current_idx : current_idx + num_indices] = seg_arr\n",
    "                    current_idx += num_indices\n",
    "\n",
    "            os.remove(nndatapath)\n",
    "\n",
    "    #                 img_arr = infile[\"img\"][:]\n",
    "    #                 seg_arr = infile[\"seg\"][:]\n",
    "    #                 weight_arr_list = []\n",
    "    #                 for item in weight_grid_list:\n",
    "    #                     weight_arr_list.append(infile[\"weight_\" + str(item)][:])\n",
    "    #             num_indices = img_arr.shape[0]\n",
    "    #             with h5py.File(nnoutputpath,\"a\") as outfile:\n",
    "    #                 outfile[\"img\"][current_idx:current_idx+num_indices] = img_arr\n",
    "    #                 outfile[\"seg\"][current_idx:current_idx+num_indices] = seg_arr\n",
    "    #                 for i,item in enumerate(weight_grid_list):\n",
    "    #                     outfile[\"weight_\" + str(item)][current_idx:current_idx+num_indices] = weight_arr_list[i]\n",
    "    #             current_idx += num_indices\n",
    "    #             os.remove(nndatapath)\n",
    "\n",
    "    #     def export_data(self,dask_controller):\n",
    "    def export_data(self, dask_controller, chunk_size=250):\n",
    "\n",
    "        dask_controller.futures = {}\n",
    "        output_meta_handle = pandas_hdf5_handler(self.metapath)\n",
    "        all_output_dfs = {}\n",
    "\n",
    "        for output_name, _ in self.import_param_dict.items():\n",
    "            output_df = []\n",
    "\n",
    "            for input_path, param_dict in self.import_param_dict[output_name].items():\n",
    "                input_meta_handle = pandas_hdf5_handler(input_path + \"/metadata.hdf5\")\n",
    "\n",
    "                num_samples = param_dict[\"Maximum Samples per Dataset:\"]\n",
    "                feature_channel = param_dict[\"Feature Channel:\"]\n",
    "                t_range = param_dict[\"Timepoint Range:\"]\n",
    "\n",
    "                kymodf = input_meta_handle.read_df(\"kymograph\", read_metadata=True)\n",
    "                kymodf[\"filepath\"] = input_path\n",
    "                trenchdf = kymodf.reset_index(inplace=False)\n",
    "                trenchdf = trenchdf.set_index(\n",
    "                    [\"filepath\", \"trenchid\", \"timepoints\"],\n",
    "                    drop=True,\n",
    "                    append=False,\n",
    "                    inplace=False,\n",
    "                )\n",
    "                trenchdf = trenchdf.sort_index()\n",
    "                trenchdf = trenchdf.loc[\n",
    "                    pd.IndexSlice[:, :, t_range[0] : t_range[1] + 1], :\n",
    "                ]\n",
    "\n",
    "                trenchdf_subset = trenchdf.sample(n=num_samples)\n",
    "                filedf_subset = trenchdf_subset.reset_index(inplace=False)\n",
    "                filedf_subset = filedf_subset.set_index(\n",
    "                    [\"filepath\", \"File Index\", \"File Trench Index\"],\n",
    "                    drop=True,\n",
    "                    append=False,\n",
    "                    inplace=False,\n",
    "                )\n",
    "                filedf_subset = filedf_subset.sort_index()\n",
    "                output_df.append(filedf_subset)\n",
    "            output_df = pd.concat(output_df)\n",
    "            output_meta_handle.write_df(output_name, output_df)\n",
    "            all_output_dfs[output_name] = output_df\n",
    "\n",
    "        for output_name in all_output_dfs.keys():\n",
    "\n",
    "            output_df = all_output_dfs[output_name]\n",
    "\n",
    "            ## split into equal computation chunks here\n",
    "\n",
    "            chunk_idx_list = []\n",
    "            for chunk_idx, init_idx in enumerate(range(0, len(output_df), chunk_size)):\n",
    "                future = dask_controller.daskclient.submit(\n",
    "                    self.export_chunk,\n",
    "                    output_name,\n",
    "                    init_idx,\n",
    "                    chunk_size,\n",
    "                    chunk_idx,\n",
    "                    retries=1,\n",
    "                )\n",
    "                dask_controller.futures[\"Chunk Number: \" + str(chunk_idx)] = future\n",
    "                chunk_idx_list.append(chunk_idx)\n",
    "\n",
    "            init_idx_list = dask_controller.daskclient.gather(\n",
    "                [\n",
    "                    dask_controller.futures[\"Chunk Number: \" + str(chunk_idx)]\n",
    "                    for chunk_idx in chunk_idx_list\n",
    "                ]\n",
    "            )\n",
    "            self.gather_chunks(output_name, init_idx_list, chunk_idx_list, chunk_size)\n",
    "\n",
    "\n",
    "#         outputdf = filedf.reset_index(inplace=False)\n",
    "#         outputdf = outputdf.set_index([\"trenchid\",\"timepoints\"], drop=True, append=False, inplace=False)\n",
    "#         outputdf = outputdf.sort_index()\n",
    "\n",
    "#         del outputdf[\"File Index\"]\n",
    "#         del outputdf[\"File Trench Index\"]\n",
    "\n",
    "#         selection_keys = [\"channel\", \"fov_list\", \"t_subsample_step\", \"t_range\", \"max_trenches\", \"ttl_imgs\", \"kymograph_img_shape\"]\n",
    "#         selection = {selection_keys[i]:item for i,item in enumerate(selection)}\n",
    "#         selection[\"experiment_name\"],selection[\"data_name\"] = (self.experimentname, dataname)\n",
    "#         selection[\"W0 List\"], selection[\"Wm Sigma List\"] = (self.grid_dict['W0 (Border Region Weight):'],self.grid_dict['Wm Sigma (Border Region Spread):'])\n",
    "\n",
    "#         output_metadata = {\"nndataset\" : selection}\n",
    "\n",
    "#         segparampath = datapath + \"/fluorescent_segmentation.par\"\n",
    "#         with open(segparampath, 'rb') as infile:\n",
    "#             seg_param_dict = pkl.load(infile)\n",
    "\n",
    "#         output_metadata[\"segmentation\"] = seg_param_dict\n",
    "\n",
    "#         input_meta_handle = pandas_hdf5_handler(datapath + \"/metadata.hdf5\")\n",
    "#         for item in [\"global\",\"kymograph\"]:\n",
    "#             indf = input_meta_handle.read_df(item,read_metadata=True)\n",
    "#             output_metadata[item] = indf.metadata\n",
    "\n",
    "#         output_meta_handle.write_df(selectionname,outputdf,metadata=output_metadata)\n",
    "\n",
    "#         file_idx_list = dask_controller.daskclient.gather([dask_controller.futures[\"File Number: \" + str(file_idx)] for file_idx in filelist])\n",
    "#         self.gather_chunks(outputdf,output_metadata,selectionname,file_idx_list,weight_grid_list)\n",
    "\n",
    "\n",
    "#     def display_grid(self):\n",
    "#         tab_dict = {'W0 (Border Region Weight):':[1., 3., 5., 10.],'Wm Sigma (Border Region Spread):':[1., 2., 3., 4., 5.]}\n",
    "#         children = [ipyw.SelectMultiple(options=val,value=(val[1],),description=key,disabled=False) for key,val in tab_dict.items()]\n",
    "#         self.tab = ipyw.Tab()\n",
    "#         self.tab.children = children\n",
    "#         for i,key in enumerate(tab_dict.keys()):\n",
    "#             self.tab.set_title(i, key[:-1])\n",
    "#         return self.tab\n",
    "\n",
    "#     def get_grid_params(self):\n",
    "#         if hasattr(self,'tab'):\n",
    "#             self.grid_dict = {child.description:child.value for child in self.tab.children}\n",
    "#             delattr(self, 'tab')\n",
    "#         elif hasattr(self,'grid_dict'):\n",
    "#             pass\n",
    "#         else:\n",
    "#             raise \"No selection defined.\"\n",
    "#         print(\"======== Grid Params ========\")\n",
    "#         for key,val in self.grid_dict.items():\n",
    "#             print(key + \" \" + str(val))\n",
    "\n",
    "#     def export_all_data(self,n_workers=20,memory='4GB'):\n",
    "#         writedir(self.nndatapath,overwrite=True)\n",
    "\n",
    "#         grid_keys = self.grid_dict.keys()\n",
    "#         grid_combinations = list(itertools.product(*list(self.grid_dict.values())))\n",
    "\n",
    "#         self.data_augmentation = data_augmentation()\n",
    "\n",
    "#         dask_cont = dask_controller(walltime='01:00:00',local=False,n_workers=n_workers,memory=memory)\n",
    "#         dask_cont.startdask()\n",
    "# #         dask_cont.daskcluster.start_workers()\n",
    "#         dask_cont.displaydashboard()\n",
    "\n",
    "#         try:\n",
    "#             for selectionname in [\"train\",\"test\",\"val\"]:\n",
    "#                 if selectionname == \"train\":\n",
    "#                     self.export_data(selectionname,dask_cont,grid_combinations,augment=True)\n",
    "#        dataloader         else:\n",
    "#                     self.export_data(selectionname,dask_cont,grid_combinations,augment=False)\n",
    "#             dask_cont.shutdown()\n",
    "#         except:\n",
    "#             dask_cont.shutdown()\n",
    "#             raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = UNet_Training_DataLoader(\n",
    "    nndatapath=\"/n/scratch2/de64/nntest7\",\n",
    "    experimentname=\"First NN\",\n",
    "    input_paths=[\"/n/scratch2/de64/2019-05-31_validation_data\"],\n",
    "    output_modes=[\"a\", \"m\"],\n",
    "    num_epochs=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader.inter_get_selection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader.get_import_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader.export_data(dask_controller, chunk_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(\"/n/scratch2/de64/nntest7/train.hdf5\", \"r\") as infile:\n",
    "    data = infile[\"2019-05-31_validation_data/m/epoch_0/img\"][:1000]\n",
    "    segdata = infile[\"2019-05-31_validation_data/m/epoch_0/seg\"][:1000]\n",
    "    data1 = infile[\"2019-05-31_validation_data/m/epoch_1/img\"][:1000]\n",
    "    segdata1 = infile[\"2019-05-31_validation_data/m/epoch_1/seg\"][:1000]\n",
    "#     img_arr = infile[\"2019-05-31_validation_data/img\"][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "matplotlib.rcParams[\"figure.figsize\"] = [20, 10]\n",
    "idx = 110\n",
    "plt.imshow(data[idx, 0])\n",
    "plt.show()\n",
    "plt.imshow(segdata[idx, 0])\n",
    "plt.show()\n",
    "plt.imshow(data1[idx, 0])\n",
    "plt.show()\n",
    "plt.imshow(segdata1[idx, 0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader.import_param_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
