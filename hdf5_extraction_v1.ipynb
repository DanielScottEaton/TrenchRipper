{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from nd2reader import ND2Reader\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import h5py\n",
    "from tifffile import imsave\n",
    "matplotlib.rcParams['figure.figsize'] = [14, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1: ND2 to hdf5\n",
    "\n",
    "I'd like the first step in the pipeline to convert the entire file to hdf5 so we can throw the nd2 away (maybe keep metadata)\n",
    "\n",
    "I need both a slow local version with parallelization (low priority) and one that can be distributed to slurm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask.distributed import Client,progress\n",
    "from dask_jobqueue import SLURMCluster\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hdf5_fov_extractor:\n",
    "    def __init__(self,nd2filename,hdf5path):\n",
    "        self.nd2filename = nd2filename\n",
    "        self.hdf5path = hdf5path\n",
    "    def extract_fov(self,fovnum):\n",
    "        nd2file = ND2Reader(self.nd2filename)\n",
    "        metadata = nd2file.metadata\n",
    "        with h5py.File(self.hdf5path + \"/fov_\" + str(fovnum) + \".hdf5\", \"w\") as h5pyfile:\n",
    "            for i,channel in enumerate(nd2file.metadata[\"channels\"]):\n",
    "                y_dim = metadata['height']\n",
    "                x_dim = metadata['width']\n",
    "                t_dim = len(nd2file.metadata['frames'])\n",
    "                hdf5_dataset = h5pyfile.create_dataset(\"channel_\" + str(channel),\\\n",
    "                                (x_dim,y_dim,t_dim), dtype='uint16')\n",
    "                for frame in nd2file.metadata['frames']:\n",
    "                    nd2_image = nd2file.get_frame_2D(c=i, t=frame, v=fovnum)\n",
    "                    hdf5_dataset[:,:,int(frame)] = nd2_image\n",
    "        nd2file.close()\n",
    "\n",
    "class tiff_fov_extractor:\n",
    "    def __init__(self,nd2filename,tiffpath):\n",
    "        self.nd2filename = nd2filename\n",
    "        self.tiffpath = tiffpath\n",
    "    def writedir(self,directory,overwrite=False):\n",
    "        if overwrite:\n",
    "            if os.path.exists(directory):\n",
    "                shutil.rmtree(directory)\n",
    "            os.makedirs(directory)\n",
    "        else:\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "    def extract_fov(self,fovnum):\n",
    "        nd2file = ND2Reader(self.nd2filename)\n",
    "        metadata = nd2file.metadata\n",
    "        for i,channel in enumerate(nd2file.metadata[\"channels\"]):\n",
    "            t_dim = len(nd2file.metadata['frames'])\n",
    "            dirpath = self.tiffpath + \"/fov_\" + str(fovnum) + \"/\" + channel + \"/\"\n",
    "            self.writedir(dirpath,overwrite=True)\n",
    "            for frame in nd2file.metadata['frames']:\n",
    "                filepath = dirpath + \"t_\" + str(frame) + \".tif\"\n",
    "                nd2_image = nd2file.get_frame_2D(c=i, t=frame, v=fovnum)\n",
    "                imsave(filepath, nd2_image)\n",
    "        nd2file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hdf5writer:\n",
    "    def __init__(self,nd2filename,outputpath,n_workers=6,local=True,queue=\"short\",\\\n",
    "                 walltime='01:30:00',cores=1,processes=1,memory='6GB'):\n",
    "        self.nd2filename = nd2filename\n",
    "        self.outputpath = outputpath\n",
    "        self.local = local\n",
    "        self.n_workers = n_workers\n",
    "        self.walltime = walltime\n",
    "        self.queue = queue\n",
    "        self.processes = processes\n",
    "        self.memory = memory\n",
    "        self.cores = cores\n",
    "    def writedir(self,directory):\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "    def startdask(self):\n",
    "        if self.local:\n",
    "            self.daskclient = Client()\n",
    "            self.daskclient.cluster.scale(self.n_workers)\n",
    "        else:\n",
    "            # note the specifed walltime, don't use too much or too little, 01:30:00 is a good baseline, \n",
    "            # you just need enough time to finish 'gathering' to props_all before the jobs die\n",
    "            # you can always spin up more jobs later\n",
    "            # you will launch many jobs, so you don't need multiple processes, a lot of ram or multiple threads\n",
    "            self.daskcluster = SLURMCluster(queue=self.queue,walltime=self.walltime,\\\n",
    "                                   processes=self.processes,memory=self.memory,\n",
    "                                  cores=self.cores)\n",
    "            self.workers = self.daskcluster.start_workers(self.n_workers)\n",
    "            self.daskclient = Client(self.daskcluster)\n",
    "    def printprogress(self):\n",
    "        complete = len([item for item in self.futures if item.status==\"finished\"])\n",
    "        print(str(complete) + \"/\" + str(len(self.futures)))\n",
    "    def startwritehdf5(self):\n",
    "        self.writedir(self.outputpath)\n",
    "        extractor = hdf5_fov_extractor(self.nd2filename,self.outputpath)\n",
    "        nd2file = ND2Reader(self.nd2filename)\n",
    "        self.futures = self.daskclient.map(extractor.extract_fov,nd2file.metadata['fields_of_view'])\n",
    "        nd2file.close()\n",
    "    def startwritetiff(self):\n",
    "        self.writedir(self.outputpath)\n",
    "        extractor = tiff_fov_extractor(self.nd2filename,self.outputpath)\n",
    "        nd2file = ND2Reader(self.nd2filename)\n",
    "        self.futures = self.daskclient.map(extractor.extract_fov,nd2file.metadata['fields_of_view'])\n",
    "        nd2file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer1 = hdf5writer(\"/n/scratch2/de64/for_sylvia/Bacillus_revival_12_7_2020.nd2\",\\\n",
    "                     \"/n/scratch2/de64/for_sylvia/tiff_out\",\\\n",
    "                     walltime='04:00:00',local=False,n_workers=20,memory='500MB')\n",
    "writer1.startdask()\n",
    "writer1.daskcluster.start_workers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Client</h3>\n",
       "<ul>\n",
       "  <li><b>Scheduler: </b>tcp://10.120.16.168:34015\n",
       "  <li><b>Dashboard: </b><a href='http://10.120.16.168:8787/status' target='_blank'>http://10.120.16.168:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Cluster</h3>\n",
       "<ul>\n",
       "  <li><b>Workers: </b>5</li>\n",
       "  <li><b>Cores: </b>5</li>\n",
       "  <li><b>Memory: </b>2.50 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://10.120.16.168:34015' processes=5 cores=5>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writer1.daskclient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Future: status: pending, key: extract_fov-0ee35abd91e2628e0de8852b32389495>,\n",
       " <Future: status: pending, key: extract_fov-dad4b36045180345e0ef6897efcd71ac>,\n",
       " <Future: status: pending, key: extract_fov-56a55fcd112b94b61d71818f0e7f0d3a>,\n",
       " <Future: status: pending, key: extract_fov-7603f02351f23cc2a25fa74589f12122>,\n",
       " <Future: status: pending, key: extract_fov-3896a5e72a6d5988a1523a1627702bd5>,\n",
       " <Future: status: pending, key: extract_fov-64c57478fba0c139cd3d2341e4ba9754>,\n",
       " <Future: status: pending, key: extract_fov-3bef4be4a30d101bc01a87158cd05f6e>,\n",
       " <Future: status: pending, key: extract_fov-882bda5ed6ed8645bc07b95d43209bce>,\n",
       " <Future: status: pending, key: extract_fov-f86349b2ffe7b2d51a54267e581fcfee>,\n",
       " <Future: status: pending, key: extract_fov-0e40419c31c627f74017d64148f566ff>,\n",
       " <Future: status: pending, key: extract_fov-edbbd42b72630fa500f0417d718e6b6b>,\n",
       " <Future: status: pending, key: extract_fov-5ace4c27fada10e722ba6ecf53c2ae47>,\n",
       " <Future: status: pending, key: extract_fov-100c816e77041d815e5e267b9f127b95>,\n",
       " <Future: status: pending, key: extract_fov-4a68a7e5e36a9268b9a55edaaa367da5>,\n",
       " <Future: status: pending, key: extract_fov-5124b3411becbb2ff089ad7649a909a5>,\n",
       " <Future: status: pending, key: extract_fov-0280f8cd26c2f926db51b14f72a295fe>,\n",
       " <Future: status: pending, key: extract_fov-c4d4e4570020267cc3823beb401e57ce>,\n",
       " <Future: status: pending, key: extract_fov-276e7e7559a7ccfccd7605f88c0ab012>,\n",
       " <Future: status: pending, key: extract_fov-e80c40692ba4791b926e7442d702d8ea>,\n",
       " <Future: status: pending, key: extract_fov-5600c84deae39857bb4ee509e5a3ee40>,\n",
       " <Future: status: pending, key: extract_fov-cb9f08aa256bc031b60c60039e7354e8>,\n",
       " <Future: status: pending, key: extract_fov-f47c9b8dd745b40fa26405d273d55240>,\n",
       " <Future: status: pending, key: extract_fov-7e28cbd7d0c7ad7feef683cd30021014>,\n",
       " <Future: status: pending, key: extract_fov-3a45452acc5fd733fdb5966b16a6c3bb>,\n",
       " <Future: status: pending, key: extract_fov-e6b0904b1a006606e5afd1a21b2f5c5c>,\n",
       " <Future: status: pending, key: extract_fov-0dd4a0cd545014e660145a77496996f1>,\n",
       " <Future: status: pending, key: extract_fov-e76e21da0c6f628a180eb0437dc9835e>,\n",
       " <Future: status: pending, key: extract_fov-7754e969dce93791e4b47550b5e72f59>,\n",
       " <Future: status: pending, key: extract_fov-0a20de9e7bf59b4a6e058c9a3d1f969a>,\n",
       " <Future: status: pending, key: extract_fov-0132ce2f2a1cf51cc982f78745c4fdfa>,\n",
       " <Future: status: pending, key: extract_fov-1122ac1c2dbda26afb17f9b4d460d1e9>,\n",
       " <Future: status: pending, key: extract_fov-808d07465d4d7c81b590018ad9ef0cfa>,\n",
       " <Future: status: pending, key: extract_fov-c3773e1399b2eef9d1b56b4056987ab2>,\n",
       " <Future: status: pending, key: extract_fov-ceff07c1595eda07ebde5a2e493d2e1d>,\n",
       " <Future: status: pending, key: extract_fov-d7eac7f9ea2db61d5dc685704fdf843d>,\n",
       " <Future: status: pending, key: extract_fov-3f802be5778aa79048318f95c3cb81f8>,\n",
       " <Future: status: pending, key: extract_fov-99bc2bf90131c6a07bee931433a04d39>,\n",
       " <Future: status: pending, key: extract_fov-4f8d146192e668abfc225114b3ccdeef>,\n",
       " <Future: status: pending, key: extract_fov-db892c77200c128b99f0e170cf17eb6c>,\n",
       " <Future: status: pending, key: extract_fov-fd830cc4813236bf1310de3a2dcff054>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method GraphPlot.__del__ of <distributed.bokeh.scheduler.GraphPlot object at 0x7f947877cbe0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/de64/anaconda3/envs/mothermachine/lib/python3.6/site-packages/distributed/bokeh/scheduler.py\", line 741, in __del__\n",
      "    self.scheduler.remove_plugin(self.layout)\n",
      "  File \"/home/de64/anaconda3/envs/mothermachine/lib/python3.6/site-packages/distributed/scheduler.py\", line 2172, in remove_plugin\n",
      "    self.plugins.remove(plugin)\n",
      "ValueError: list.remove(x): x not in list\n"
     ]
    }
   ],
   "source": [
    "writer1.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/de64/anaconda3/envs/mothermachine/lib/python3.6/site-packages/nd2reader/common_raw_metadata.py:94: RuntimeWarning: Reported average frame interval (119915.3 ms) doesn't match the set interval (120000.0 ms). Using the average now.\n",
      "  warnings.warn(message % (avg_interval, interval), RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "writer1.startwritetiff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/40\n"
     ]
    }
   ],
   "source": [
    "writer1.printprogress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "props = writer1.daskclient.gather(writer1.futures) #this will hang until all futures are done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer1.daskcluster.stop_workers(writer1.workers)  #this is still not working\n",
    "writer1.daskcluster.stop_all_jobs() #this seems to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[556 566 597 ... 660 676 662]\n",
      " [485 618 464 ... 680 601 710]\n",
      " [615 562 521 ... 715 713 703]\n",
      " ...\n",
      " [789 789 764 ... 745 745 797]\n",
      " [846 805 688 ... 676 778 818]\n",
      " [827 830 918 ... 778 794 797]]\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('mytestfile.hdf5', 'r') as df:\n",
    "    for fov in df:\n",
    "        for frame in df[fov]:\n",
    "            for color in df[fov+\"/\"+frame]:\n",
    "                print(df[fov+\"/\"+frame + \"/\" + color][:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
