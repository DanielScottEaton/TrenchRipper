{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from nd2reader import ND2Reader\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import h5py\n",
    "from tifffile import imsave\n",
    "matplotlib.rcParams['figure.figsize'] = [14, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1: ND2 to hdf5\n",
    "\n",
    "I'd like the first step in the pipeline to convert the entire file to hdf5 so we can throw the nd2 away (maybe keep metadata)\n",
    "\n",
    "I need both a slow local version with parallelization (low priority) and one that can be distributed to slurm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask.distributed import Client,progress\n",
    "from dask_jobqueue import SLURMCluster\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hdf5_fov_extractor:\n",
    "    def __init__(self,nd2filename,hdf5path):\n",
    "        self.nd2filename = nd2filename\n",
    "        self.hdf5path = hdf5path\n",
    "        self.writedir(hdf5path)\n",
    "    def writedir(self,directory,overwrite=False):\n",
    "        if overwrite:\n",
    "            if os.path.exists(directory):\n",
    "                shutil.rmtree(directory)\n",
    "            os.makedirs(directory)\n",
    "        else:\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "    def extract_fov(self,fovnum):\n",
    "        nd2file = ND2Reader(self.nd2filename)\n",
    "        metadata = nd2file.metadata\n",
    "        with h5py.File(self.hdf5path + \"/fov_\" + str(fovnum) + \".hdf5\", \"w\") as h5pyfile:\n",
    "            for i,channel in enumerate(nd2file.metadata[\"channels\"]):\n",
    "                y_dim = metadata['height']\n",
    "                x_dim = metadata['width']\n",
    "                t_dim = len(nd2file.metadata['frames'])\n",
    "                hdf5_dataset = h5pyfile.create_dataset(\"channel_\" + str(channel),\\\n",
    "                                (x_dim,y_dim,t_dim),chunks=(x_dim,y_dim,1),dtype='uint16')\n",
    "                for frame in nd2file.metadata['frames']:\n",
    "                    print(frame)\n",
    "                    nd2_image = nd2file.get_frame_2D(c=i, t=frame, v=fovnum)\n",
    "                    hdf5_dataset[:,:,int(frame)] = nd2_image\n",
    "        nd2file.close()\n",
    "class tiff_fov_extractor:\n",
    "    def __init__(self,nd2filename,tiffpath):\n",
    "        self.nd2filename = nd2filename\n",
    "        self.tiffpath = tiffpath\n",
    "    def writedir(self,directory,overwrite=False):\n",
    "        if overwrite:\n",
    "            if os.path.exists(directory):\n",
    "                shutil.rmtree(directory)\n",
    "            os.makedirs(directory)\n",
    "        else:\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "    def extract_fov(self,fovnum):\n",
    "        nd2file = ND2Reader(self.nd2filename)\n",
    "        metadata = nd2file.metadata\n",
    "        for i,channel in enumerate(nd2file.metadata[\"channels\"]):\n",
    "            t_dim = len(nd2file.metadata['frames'])\n",
    "            dirpath = self.tiffpath + \"/fov_\" + str(fovnum) + \"/\" + channel + \"/\"\n",
    "            self.writedir(dirpath,overwrite=True)\n",
    "            for frame in nd2file.metadata['frames']:\n",
    "                filepath = dirpath + \"t_\" + str(frame) + \".tif\"\n",
    "                nd2_image = nd2file.get_frame_2D(c=i, t=frame, v=fovnum)\n",
    "                imsave(filepath, nd2_image)\n",
    "        nd2file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_extractor = hdf5_fov_extractor(\"/n/scratch2/de64/for_sylvia/Bacillus_revival_12_7_2020.nd2\",\"/n/scratch2/de64/full_pipeline_test/hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/de64/anaconda3/envs/mothermachine/lib/python3.6/site-packages/nd2reader/common_raw_metadata.py:94: RuntimeWarning: Reported average frame interval (119915.3 ms) doesn't match the set interval (120000.0 ms). Using the average now.\n",
      "  warnings.warn(message % (avg_interval, interval), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/de64/anaconda3/envs/mothermachine/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-5-236b4960292d>\", line 1, in <module>\n",
      "    hdf5_extractor.extract_fov(1)\n",
      "  File \"<ipython-input-3-dc8a00b1a177>\", line 25, in extract_fov\n",
      "    nd2_image = nd2file.get_frame_2D(c=i, t=frame, v=fovnum)\n",
      "  File \"/home/de64/anaconda3/envs/mothermachine/lib/python3.6/site-packages/nd2reader/reader.py\", line 77, in get_frame_2D\n",
      "    return self._parser.get_image_by_attributes(t, v, c_name, z, y, x)\n",
      "  File \"/home/de64/anaconda3/envs/mothermachine/lib/python3.6/site-packages/nd2reader/parser.py\", line 98, in get_image_by_attributes\n",
      "    height, width)\n",
      "  File \"/home/de64/anaconda3/envs/mothermachine/lib/python3.6/site-packages/nd2reader/parser.py\", line 256, in _get_raw_image_data\n",
      "    data = read_chunk(self._fh, chunk)\n",
      "  File \"/home/de64/anaconda3/envs/mothermachine/lib/python3.6/site-packages/nd2reader/common.py\", line 62, in read_chunk\n",
      "    chunk_metadata = fh.read(16)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/de64/anaconda3/envs/mothermachine/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/de64/anaconda3/envs/mothermachine/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/de64/anaconda3/envs/mothermachine/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/de64/anaconda3/envs/mothermachine/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/de64/anaconda3/envs/mothermachine/lib/python3.6/inspect.py\", line 1483, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/de64/anaconda3/envs/mothermachine/lib/python3.6/inspect.py\", line 1441, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/de64/anaconda3/envs/mothermachine/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/de64/anaconda3/envs/mothermachine/lib/python3.6/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/home/de64/anaconda3/envs/mothermachine/lib/python3.6/posixpath.py\", line 389, in realpath\n",
      "    return abspath(path)\n",
      "  File \"/home/de64/anaconda3/envs/mothermachine/lib/python3.6/posixpath.py\", line 378, in abspath\n",
      "    return normpath(path)\n",
      "  File \"/home/de64/anaconda3/envs/mothermachine/lib/python3.6/posixpath.py\", line 352, in normpath\n",
      "    comps = path.split(sep)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "hdf5_extractor.extract_fov(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/de64/anaconda3/envs/mothermachine/lib/python3.6/site-packages/nd2reader/common_raw_metadata.py:94: RuntimeWarning: Reported average frame interval (119915.3 ms) doesn't match the set interval (120000.0 ms). Using the average now.\n",
      "  warnings.warn(message % (avg_interval, interval), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "*** KeyboardInterrupt exception caught in code being profiled."
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-06 s\n",
       "\n",
       "Total time: 37.7137 s\n",
       "File: <ipython-input-3-dc8a00b1a177>\n",
       "Function: extract_fov at line 14\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    14                                               def extract_fov(self,fovnum):\n",
       "    15         1     123539.0 123539.0      0.3          nd2file = ND2Reader(self.nd2filename)\n",
       "    16         1          2.0      2.0      0.0          metadata = nd2file.metadata\n",
       "    17         1      45680.0  45680.0      0.1          with h5py.File(self.hdf5path + \"/fov_\" + str(fovnum) + \".hdf5\", \"w\") as h5pyfile:\n",
       "    18         1          8.0      8.0      0.0              for i,channel in enumerate(nd2file.metadata[\"channels\"]):\n",
       "    19         1          2.0      2.0      0.0                  y_dim = metadata['height']\n",
       "    20         1          1.0      1.0      0.0                  x_dim = metadata['width']\n",
       "    21         1          2.0      2.0      0.0                  t_dim = len(nd2file.metadata['frames'])\n",
       "    22         1        598.0    598.0      0.0                  hdf5_dataset = h5pyfile.create_dataset(\"channel_\" + str(channel),                                (x_dim,y_dim,t_dim),chunks=(x_dim,y_dim,1),dtype='uint16')\n",
       "    23        53        110.0      2.1      0.0                  for frame in nd2file.metadata['frames']:\n",
       "    24        53      18139.0    342.2      0.0                      print(frame)\n",
       "    25        53    2968970.0  56018.3      7.9                      nd2_image = nd2file.get_frame_2D(c=i, t=frame, v=fovnum)\n",
       "    26        53   34556626.0 652011.8     91.6                      hdf5_dataset[:,:,int(frame)] = nd2_image\n",
       "    27                                                   nd2file.close()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext line_profiler\n",
    "%lprun -f hdf5_extractor.extract_fov hdf5_extractor.extract_fov(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hdf5writer:\n",
    "    def __init__(self,nd2filename,outputpath,n_workers=6,local=True,queue=\"short\",\\\n",
    "                 walltime='01:30:00',cores=1,processes=1,memory='6GB'):\n",
    "        self.nd2filename = nd2filename\n",
    "        self.outputpath = outputpath\n",
    "        self.local = local\n",
    "        self.n_workers = n_workers\n",
    "        self.walltime = walltime\n",
    "        self.queue = queue\n",
    "        self.processes = processes\n",
    "        self.memory = memory\n",
    "        self.cores = cores\n",
    "    def writedir(self,directory):\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "    def startdask(self):\n",
    "        if self.local:\n",
    "            self.daskclient = Client()\n",
    "            self.daskclient.cluster.scale(self.n_workers)\n",
    "        else:\n",
    "            # note the specifed walltime, don't use too much or too little, 01:30:00 is a good baseline, \n",
    "            # you just need enough time to finish 'gathering' to props_all before the jobs die\n",
    "            # you can always spin up more jobs later\n",
    "            # you will launch many jobs, so you don't need multiple processes, a lot of ram or multiple threads\n",
    "            self.daskcluster = SLURMCluster(queue=self.queue,walltime=self.walltime,\\\n",
    "                                   processes=self.processes,memory=self.memory,\n",
    "                                  cores=self.cores)\n",
    "            self.workers = self.daskcluster.start_workers(self.n_workers)\n",
    "            self.daskclient = Client(self.daskcluster)\n",
    "    def printprogress(self):\n",
    "        complete = len([item for item in self.futures if item.status==\"finished\"])\n",
    "        print(str(complete) + \"/\" + str(len(self.futures)))\n",
    "    def startwritehdf5(self):\n",
    "        self.writedir(self.outputpath)\n",
    "        extractor = hdf5_fov_extractor(self.nd2filename,self.outputpath)\n",
    "        nd2file = ND2Reader(self.nd2filename)\n",
    "        self.futures = self.daskclient.map(extractor.extract_fov,nd2file.metadata['fields_of_view'])\n",
    "        nd2file.close()\n",
    "    def startwritetiff(self):\n",
    "        self.writedir(self.outputpath)\n",
    "        extractor = tiff_fov_extractor(self.nd2filename,self.outputpath)\n",
    "        nd2file = ND2Reader(self.nd2filename)\n",
    "        self.futures = self.daskclient.map(extractor.extract_fov,nd2file.metadata['fields_of_view'])\n",
    "        nd2file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer1 = hdf5writer(\"/n/scratch2/de64/for_sylvia/Bacillus_revival_12_7_2020.nd2\",\\\n",
    "                     \"/n/scratch2/de64/for_sylvia/tiff_out\",\\\n",
    "                     walltime='04:00:00',local=False,n_workers=20,memory='500MB')\n",
    "writer1.startdask()\n",
    "writer1.daskcluster.start_workers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Client</h3>\n",
       "<ul>\n",
       "  <li><b>Scheduler: </b>tcp://10.120.16.168:34015\n",
       "  <li><b>Dashboard: </b><a href='http://10.120.16.168:8787/status' target='_blank'>http://10.120.16.168:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Cluster</h3>\n",
       "<ul>\n",
       "  <li><b>Workers: </b>5</li>\n",
       "  <li><b>Cores: </b>5</li>\n",
       "  <li><b>Memory: </b>2.50 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://10.120.16.168:34015' processes=5 cores=5>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writer1.daskclient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Future: status: pending, key: extract_fov-0ee35abd91e2628e0de8852b32389495>,\n",
       " <Future: status: pending, key: extract_fov-dad4b36045180345e0ef6897efcd71ac>,\n",
       " <Future: status: pending, key: extract_fov-56a55fcd112b94b61d71818f0e7f0d3a>,\n",
       " <Future: status: pending, key: extract_fov-7603f02351f23cc2a25fa74589f12122>,\n",
       " <Future: status: pending, key: extract_fov-3896a5e72a6d5988a1523a1627702bd5>,\n",
       " <Future: status: pending, key: extract_fov-64c57478fba0c139cd3d2341e4ba9754>,\n",
       " <Future: status: pending, key: extract_fov-3bef4be4a30d101bc01a87158cd05f6e>,\n",
       " <Future: status: pending, key: extract_fov-882bda5ed6ed8645bc07b95d43209bce>,\n",
       " <Future: status: pending, key: extract_fov-f86349b2ffe7b2d51a54267e581fcfee>,\n",
       " <Future: status: pending, key: extract_fov-0e40419c31c627f74017d64148f566ff>,\n",
       " <Future: status: pending, key: extract_fov-edbbd42b72630fa500f0417d718e6b6b>,\n",
       " <Future: status: pending, key: extract_fov-5ace4c27fada10e722ba6ecf53c2ae47>,\n",
       " <Future: status: pending, key: extract_fov-100c816e77041d815e5e267b9f127b95>,\n",
       " <Future: status: pending, key: extract_fov-4a68a7e5e36a9268b9a55edaaa367da5>,\n",
       " <Future: status: pending, key: extract_fov-5124b3411becbb2ff089ad7649a909a5>,\n",
       " <Future: status: pending, key: extract_fov-0280f8cd26c2f926db51b14f72a295fe>,\n",
       " <Future: status: pending, key: extract_fov-c4d4e4570020267cc3823beb401e57ce>,\n",
       " <Future: status: pending, key: extract_fov-276e7e7559a7ccfccd7605f88c0ab012>,\n",
       " <Future: status: pending, key: extract_fov-e80c40692ba4791b926e7442d702d8ea>,\n",
       " <Future: status: pending, key: extract_fov-5600c84deae39857bb4ee509e5a3ee40>,\n",
       " <Future: status: pending, key: extract_fov-cb9f08aa256bc031b60c60039e7354e8>,\n",
       " <Future: status: pending, key: extract_fov-f47c9b8dd745b40fa26405d273d55240>,\n",
       " <Future: status: pending, key: extract_fov-7e28cbd7d0c7ad7feef683cd30021014>,\n",
       " <Future: status: pending, key: extract_fov-3a45452acc5fd733fdb5966b16a6c3bb>,\n",
       " <Future: status: pending, key: extract_fov-e6b0904b1a006606e5afd1a21b2f5c5c>,\n",
       " <Future: status: pending, key: extract_fov-0dd4a0cd545014e660145a77496996f1>,\n",
       " <Future: status: pending, key: extract_fov-e76e21da0c6f628a180eb0437dc9835e>,\n",
       " <Future: status: pending, key: extract_fov-7754e969dce93791e4b47550b5e72f59>,\n",
       " <Future: status: pending, key: extract_fov-0a20de9e7bf59b4a6e058c9a3d1f969a>,\n",
       " <Future: status: pending, key: extract_fov-0132ce2f2a1cf51cc982f78745c4fdfa>,\n",
       " <Future: status: pending, key: extract_fov-1122ac1c2dbda26afb17f9b4d460d1e9>,\n",
       " <Future: status: pending, key: extract_fov-808d07465d4d7c81b590018ad9ef0cfa>,\n",
       " <Future: status: pending, key: extract_fov-c3773e1399b2eef9d1b56b4056987ab2>,\n",
       " <Future: status: pending, key: extract_fov-ceff07c1595eda07ebde5a2e493d2e1d>,\n",
       " <Future: status: pending, key: extract_fov-d7eac7f9ea2db61d5dc685704fdf843d>,\n",
       " <Future: status: pending, key: extract_fov-3f802be5778aa79048318f95c3cb81f8>,\n",
       " <Future: status: pending, key: extract_fov-99bc2bf90131c6a07bee931433a04d39>,\n",
       " <Future: status: pending, key: extract_fov-4f8d146192e668abfc225114b3ccdeef>,\n",
       " <Future: status: pending, key: extract_fov-db892c77200c128b99f0e170cf17eb6c>,\n",
       " <Future: status: pending, key: extract_fov-fd830cc4813236bf1310de3a2dcff054>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method GraphPlot.__del__ of <distributed.bokeh.scheduler.GraphPlot object at 0x7f947877cbe0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/de64/anaconda3/envs/mothermachine/lib/python3.6/site-packages/distributed/bokeh/scheduler.py\", line 741, in __del__\n",
      "    self.scheduler.remove_plugin(self.layout)\n",
      "  File \"/home/de64/anaconda3/envs/mothermachine/lib/python3.6/site-packages/distributed/scheduler.py\", line 2172, in remove_plugin\n",
      "    self.plugins.remove(plugin)\n",
      "ValueError: list.remove(x): x not in list\n"
     ]
    }
   ],
   "source": [
    "writer1.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/de64/anaconda3/envs/mothermachine/lib/python3.6/site-packages/nd2reader/common_raw_metadata.py:94: RuntimeWarning: Reported average frame interval (119915.3 ms) doesn't match the set interval (120000.0 ms). Using the average now.\n",
      "  warnings.warn(message % (avg_interval, interval), RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "writer1.startwritetiff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/40\n"
     ]
    }
   ],
   "source": [
    "writer1.printprogress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "props = writer1.daskclient.gather(writer1.futures) #this will hang until all futures are done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer1.daskcluster.stop_workers(writer1.workers)  #this is still not working\n",
    "writer1.daskcluster.stop_all_jobs() #this seems to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[556 566 597 ... 660 676 662]\n",
      " [485 618 464 ... 680 601 710]\n",
      " [615 562 521 ... 715 713 703]\n",
      " ...\n",
      " [789 789 764 ... 745 745 797]\n",
      " [846 805 688 ... 676 778 818]\n",
      " [827 830 918 ... 778 794 797]]\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('mytestfile.hdf5', 'r') as df:\n",
    "    for fov in df:\n",
    "        for frame in df[fov]:\n",
    "            for color in df[fov+\"/\"+frame]:\n",
    "                print(df[fov+\"/\"+frame + \"/\" + color][:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
